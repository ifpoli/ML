{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ветренко Полина, М8О-306Б-17\n",
    "\n",
    "**Импорт необходимых библиотек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (18,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импорт данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1351,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSING_PATH = \"data/\" \n",
    "def load_data(housing_path=HOUSING_PATH):    \n",
    "    csv_path = os.path.join(housing_path, \"winequality-red.csv\")   \n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета: (1599, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 1352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data()\n",
    "\n",
    "print(\"Размер датасета:\", data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixed acidity': 96, 'volatile acidity': 143, 'citric acid': 80, 'residual sugar': 91, 'chlorides': 153, 'free sulfur dioxide': 60, 'total sulfur dioxide': 144, 'density': 436, 'pH': 89, 'sulphates': 96, 'alcohol': 65, 'quality': 6}\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict(data.nunique())\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23   4     1       3   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21   3     1       2   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23   1     1       4   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29   3     5       5   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31   1     6       3   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на параметр, по которому будем делить данные на тестовые и тренировочные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Минимальный процент алкоголя: 8.4\n",
      "Максимальный процент алкоголя: 14.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Минимальный процент алкоголя:\", data.alcohol.min())\n",
    "print(\"Максимальный процент алкоголя:\", data.alcohol.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[data.alcohol <= 12]\n",
    "data_test = data[data.alcohol > 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = data_train['quality']\n",
    "Y_test = data_test['quality']\n",
    "\n",
    "X_train = data_train.drop('quality', axis = 1)\n",
    "X_test = data_test.drop('quality', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество данных для тестового набора составляет: 8.818011257035648%\n"
     ]
    }
   ],
   "source": [
    "print(\"Количество данных для тестового набора составляет:\", str(100 * data_test.shape[0] / data.shape[0]) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv(\"data/train.csv\")\n",
    "data_test.to_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_features_std = StandardScaler() \n",
    "\n",
    "X_train = scale_features_std.fit_transform(X_train) \n",
    "X_test = scale_features_std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1656,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train.csv\")[:1000]\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train_data.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "test_data.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1657,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 1657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные с 8.4 по 12.0 % алкоголя\n"
     ]
    }
   ],
   "source": [
    "alcohol_validate = train_data['alcohol'].to_numpy()\n",
    "print(\"Данные с\", alcohol_validate.min(), \"по\", alcohol_validate.max(), \"% алкоголя\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_data['quality'].to_numpy()\n",
    "Y_test = test_data['quality'].to_numpy()\n",
    "\n",
    "X_train = train_data.drop('quality', axis = 1)\n",
    "X_test = test_data.drop('quality', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_features_std = MinMaxScaler()\n",
    "\n",
    "X_train = scale_features_std.fit_transform(X_train.to_numpy()) \n",
    "X_test = scale_features_std.transform(X_test.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загружаем библиотеки и создаем необходимые метрики**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    TN = np.logical_not(Y_val | Y_pred).sum()\n",
    "    return (TP + TN) / len(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    return TP / Y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    return TP / Y_val.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_metric(Y_val, Y_pred):\n",
    "    precision = Precision(Y_val, Y_pred)\n",
    "    recall = Recall(Y_val, Y_pred)\n",
    "    return 2.0 * recall * precision / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_norm(vector):\n",
    "    return (vector**2).sum()\n",
    "\n",
    "def L1_norm(vector):\n",
    "    return np.abs(vec).sum()\n",
    "def L2_grad(vector):\n",
    "    return vector\n",
    "\n",
    "def L1_grad(vector):\n",
    "    return vector / np.abs(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this exponent numericall stable\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "def logit_loss(wx, y_real):\n",
    "    return np.log(1.0 + np.exp(-wx*y_real)).sum()\n",
    "\n",
    "def logit_grad(x, y, w):\n",
    "    koeff = (y * sigmoid(-y*x.dot(w)))\n",
    "    koeff = koeff.reshape((koeff.shape[0], 1)) # make a column\n",
    "    return -(koeff * x).sum(axis = 0) # full gradient - sum of gradients on ever x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, speed, gradient_func, regulasator=None, \n",
    "                 C=10.0, eps = 0.001, maxsteps=250):\n",
    "        self.speed = speed\n",
    "        self.function = gradient_func\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "        if regulasator == \"l1\":\n",
    "            self.regulasator = lambda w:  L1_grad(w) / C\n",
    "        elif regulasator == \"l2\":\n",
    "            self.regulasator = lambda w: L2_grad(w) / C\n",
    "        else:\n",
    "            self.regulasator = lambda w: 0.0\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        # init w0\n",
    "        w0 = np.zeros(X_train.shape[1])\n",
    "        w = np.random.random(X_train.shape[1])\n",
    "        k = 1\n",
    "        while np.linalg.norm(w - w0) > self.eps and k <= self.maxsteps:\n",
    "            w0 = w\n",
    "            temp = self.speed * ((1 / k)**0.5) # like vowpal step temp\n",
    "            w = w - temp*(self.function(X_train, Y_train, w) + self.regulasator(w))\n",
    "            k += 1\n",
    "            \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with 2 classes: 0 and 1.\n",
    "class BinaryLogisticRegression:\n",
    "    # main params\n",
    "    def __init__(self, speed = 1.5, reg_type=None, C=2.0, eps=0.001, maxsteps=200):\n",
    "        # init solver\n",
    "        self.solver = GradientDescent(speed, logit_grad, reg_type, C, eps, maxsteps)\n",
    "        # init weight  variable\n",
    "        self.w = None\n",
    "        \n",
    "    # training\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # convert 0 to -1 for algo\n",
    "        Y = np.array(Y_train)\n",
    "        Y[Y_train == 0] = -1\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_train.shape[0], 1))\n",
    "        X = np.hstack((x0, X_train))\n",
    "        # train weight by gradient descent\n",
    "        self.w = self.solver.fit(X, Y)\n",
    "        return self\n",
    "    \n",
    "    # returns predictes classes\n",
    "    def predict(self, X_val, border = 0):\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_val.shape[0], 1))\n",
    "        X = np.hstack((x0, X_val))\n",
    "        # <w, x> product for all examples\n",
    "        Xw = X.dot(self.w)\n",
    "        # make predict: 0 - negative, 1 - positive\n",
    "        Y_pred = np.zeros(Xw.shape).astype(np.int8)\n",
    "        # a(x) = [<w,x> > t], t - border\n",
    "        Y_pred[Xw >= border] = 1\n",
    "        return Y_pred\n",
    "    \n",
    "    # probs of positive class\n",
    "    def predict_proba(self, X_val):\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_val.shape[0], 1))\n",
    "        X = np.hstack((x0, X_val))\n",
    "        # <w, x> product for all examples\n",
    "        Xw = X.dot(self.w)\n",
    "        # return proba\n",
    "        return sigmoid(Xw)\n",
    "    \n",
    "    # compute metrics\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.w\n",
    "\n",
    "    # for fun\n",
    "    def __str__(self):\n",
    "        return \"Logistic Regression model with gradient descent!\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Logistic Regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решающее дерево**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNode:\n",
    "    def __init__(self, idxs=None, pos=None, neg=None, c=None):\n",
    "        self.predicat = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.positives = pos\n",
    "        self.negatives = neg\n",
    "        self.c = c\n",
    "        self.idxs = idxs\n",
    "\n",
    "    # setters:\n",
    "    def set_left(self, left_node):\n",
    "        self.left = left_node\n",
    "\n",
    "    def set_idxs(self, idxs):\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def set_right(self, right_node):\n",
    "        self.right = right_node\n",
    "\n",
    "    def set_predicat(self, predicat):\n",
    "        self.predicat = predicat\n",
    "\n",
    "    def set_class(self, c):\n",
    "        self.c = c\n",
    "        \n",
    "    def set_positives(self, positives):\n",
    "        self.positives = positives\n",
    "        \n",
    "    def set_negatives(self, negatives):\n",
    "        self.negatives = negatives\n",
    "\n",
    "    #getters:\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "\n",
    "    def get_class(self):\n",
    "        return self.c\n",
    "\n",
    "    def get_idxs(self):\n",
    "        return self.idxs\n",
    "    \n",
    "    def get_positives(self):\n",
    "        return self.positives\n",
    "        \n",
    "    def get_negatives(self):\n",
    "        return self.negatives\n",
    "    \n",
    "    def get_len(self):\n",
    "        return self.idxs.shape[0]\n",
    "\n",
    "    #checkers:\n",
    "    def is_leaf(self):\n",
    "        return self.predicat is None\n",
    "\n",
    "    def is_inner(self):\n",
    "        return not self.is_leaf()\n",
    "    \n",
    "    def make_leaf(self):\n",
    "        self.predicat = None\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bingini(*args):\n",
    "    # for binary classification: p(-) = 1 - p(+)=>\n",
    "    # Gini = 2 * p(+) * (1 - p(+))\n",
    "    if len(args) == 2:\n",
    "        p = args[0] / (args[1]+args[0])\n",
    "    else:\n",
    "        p = args[0].sum() / args[0].shape[0]\n",
    "    return 2 * p * (1 - p)\n",
    "\n",
    "def binentropy(*args):\n",
    "    # for binary classification: p(-) = 1 - p(+)=>\n",
    "    # Entropy = -p(+)log(p(+)) - (1 - p(+))log(1 - p(+))\n",
    "    if len(args) == 2:\n",
    "        p = args[0] / (args[1]+args[0])\n",
    "    else:\n",
    "        p = args[0].sum() / args[0].shape[0]\n",
    "    return -p*np.log(p) - (1 - p)*np.log(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDescisionTree:\n",
    "    def __init__(self, criteria=bingini, pruning_cost=None, min_samples_split=2, random_sub_num=None):\n",
    "        self.CATEGORICAL_LEN = 10\n",
    "        self.is_categorical = None\n",
    "        self.categorical_vals = {}\n",
    "        self.H = criteria\n",
    "        self.root = None\n",
    "        self.min_split = min_samples_split\n",
    "        if random_sub_num is None:\n",
    "            self.random_subspace = False\n",
    "        else:\n",
    "            self.random_subspace = True\n",
    "            self.feature_num = random_sub_num\n",
    "        if pruning_cost is None:\n",
    "            self.pruning = False\n",
    "        else:\n",
    "            self.pruning = True\n",
    "            self.alpha = pruning_cost\n",
    "            \n",
    "    def node_classify(self, node):\n",
    "        # set class num node:\n",
    "        positives = node.get_positives()\n",
    "        negatives = node.get_negatives()\n",
    "        if positives >= negatives:\n",
    "            node.set_class(1)\n",
    "        else:\n",
    "            node.set_class(0)\n",
    "            \n",
    "    # create root and recursive building tree\n",
    "    def build_tree(self):\n",
    "        # create root with all nums\n",
    "        self.root = BinaryNode(np.arange(self.Y.shape[0]))\n",
    "        self.root.set_positives(self.Y.sum())\n",
    "        negatives = self.root.get_len() - self.root.get_positives()\n",
    "        self.root.set_negatives(negatives)\n",
    "        # recursive function of creation\n",
    "        self.recursive_creation(self.root)\n",
    "            \n",
    "            \n",
    "    # may be modifed\n",
    "    def stop_criteria(self, node):\n",
    "        # if num of eelems in node less than min required for split => 1\n",
    "        if node.get_len() < self.min_split:\n",
    "            return True\n",
    "        # if all elems in node has only one class => 1\n",
    "        positives = node.get_positives()\n",
    "        negatives = node.get_negatives()\n",
    "        return (negatives==0 or positives==0)\n",
    "    \n",
    "    # union of search_best_split() and split_node()\n",
    "    def search_best_split(self, node):\n",
    "        X_iter = self.X[node.get_idxs()]\n",
    "        Y_iter = self.Y[node.get_idxs()]\n",
    "        # compute node info criteria\n",
    "        positiv = node.get_positives()\n",
    "        negativ = node.get_negatives()\n",
    "        node_info = self.H(positiv, negativ)\n",
    "        # best params\n",
    "        best_gain = 0.0\n",
    "        best_j, best_t = 0, 0.0\n",
    "        # search in all features:\n",
    "        if self.random_subspace:\n",
    "            # get random permutation\n",
    "            features = np.random.permutation(self.X.shape[1])\n",
    "            # stay only feature_num random features\n",
    "            features = features[:self.feature_num]\n",
    "        # else search by all features\n",
    "        else:\n",
    "            features = range(self.X.shape[1])\n",
    "\n",
    "        for j in features:\n",
    "            column = X_iter[:, j]\n",
    "            # fast search if categorical:\n",
    "            if self.is_categorical[j]:\n",
    "                possible_vals = self.categorical_vals[j]\n",
    "                for i in range(1, possible_vals.shape[0]):\n",
    "                    mask = column < possible_vals[i]\n",
    "                    Y_r = Y_iter[mask]\n",
    "                    if Y_r.shape[0] == 0 or Y_r.shape[0] == Y_iter.shape[0]:\n",
    "                        continue\n",
    "                    right_pos = Y_r.sum()\n",
    "                    right_neg = Y_r.shape[0] - right_pos\n",
    "                    right_gini = self.H(right_pos, right_neg)\n",
    "                    left_gini = self.H(positiv - right_pos, negativ - right_neg)\n",
    "                    # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
    "                    gain = node_info\n",
    "                    gain -= (Y_r.shape[0]*right_gini/node.get_len())\n",
    "                    gain -= (1 - Y_r.shape[0]/node.get_len())*left_gini\n",
    "                    if gain > best_gain:\n",
    "                        best_t = possible_vals[i]\n",
    "                        best_j = j\n",
    "                        best_gain = gain  \n",
    "                continue\n",
    "            # else standart search:\n",
    "            sorted_col = np.argsort(column)\n",
    "            right_neg = 0\n",
    "            right_pos = 0\n",
    "            last_t = column[sorted_col[0]]\n",
    "            for i in range(1, column.shape[0]):\n",
    "                if Y_iter[sorted_col[i-1]]:\n",
    "                    right_pos += 1\n",
    "                else:\n",
    "                    right_neg += 1\n",
    "                    \n",
    "                idx = sorted_col[i]\n",
    "                if column[idx] == last_t:\n",
    "                    continue\n",
    "                \n",
    "                last_t = column[idx]\n",
    "                # compute gain:\n",
    "                right_gini = self.H(right_pos, right_neg)\n",
    "                left_gini = self.H(positiv - right_pos, negativ - right_neg)\n",
    "                # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
    "                gain = node_info\n",
    "                gain -= (i*right_gini/node.get_len()) + (1 - i/node.get_len())*left_gini\n",
    "                # needs best gain split\n",
    "                if gain > best_gain:\n",
    "                    best_t = column[idx]\n",
    "                    best_j = j\n",
    "                    best_gain = gain\n",
    "                    \n",
    "        if best_gain > 0.0:\n",
    "            return best_j, best_t\n",
    "    \n",
    "    # create 2 new nodes: left and right\n",
    "    def split_node(self, node, j, t):\n",
    "        # set predicat rule for node\n",
    "        predicat = lambda x: x[j] < t\n",
    "        node.set_predicat(predicat)\n",
    "        # get split mask\n",
    "        column = self.X[node.get_idxs(), j]\n",
    "        mask = column < t\n",
    "        # make idxs for left and right\n",
    "        right_idxs = node.get_idxs()[mask]\n",
    "        left_idxs = node.get_idxs()[np.logical_not(mask)]\n",
    "        #compute positives:\n",
    "        right_pos = self.Y[right_idxs].sum()\n",
    "        left_pos = self.Y[left_idxs].sum()\n",
    "        # compute negatives\n",
    "        right_neg = right_idxs.shape[0] - right_pos\n",
    "        left_neg = left_idxs.shape[0] - left_pos\n",
    "        # create nodes\n",
    "        node.set_left(BinaryNode(left_idxs, left_pos, left_neg))\n",
    "        node.set_right(BinaryNode(right_idxs, right_pos, right_neg))\n",
    "    \n",
    "    # recursive function for nodes:\n",
    "    def recursive_creation(self, node):\n",
    "        # classify another node:\n",
    "        self.node_classify(node)\n",
    "        # stop criteria for building\n",
    "        if self.stop_criteria(node):\n",
    "            return\n",
    "        #else find best split\n",
    "        jt = self.search_best_split(node)\n",
    "        # if we cant find best split - stop\n",
    "        if jt is None:\n",
    "            return\n",
    "\n",
    "        # split node for 2 child:\n",
    "        self.split_node(node, *jt)\n",
    "        # start recursion for left:\n",
    "        self.recursive_creation(node.get_left())\n",
    "        # for right:\n",
    "        self.recursive_creation(node.get_right())\n",
    "    \n",
    "    def tree_pruning(self, node):\n",
    "        # this node R_a(t) computing:\n",
    "        # R = sum([y != c]) / |N|\n",
    "        if node.get_class():\n",
    "            R_a = node.get_negatives() / node.get_len()\n",
    "        else:\n",
    "            R_a = node.get_positives() / node.get_len()\n",
    "        # R_a(t) = R(t) + a\n",
    "        R_a += self.alpha\n",
    "        # at first go while not leaf:\n",
    "        if node.is_leaf():\n",
    "            # return R_a(leaf)\n",
    "            return R_a\n",
    "        # R_a(Tl) and R_a(Tr)\n",
    "        R_al = self.tree_pruning(node.get_left())\n",
    "        R_ar = self.tree_pruning(node.get_right())\n",
    "        # if R_a(t) < R_a(T) => pruning\n",
    "        if R_a <= R_al + R_ar:\n",
    "            node.make_leaf()\n",
    "            return R_a\n",
    "        # else do nothing\n",
    "        return R_al + R_ar\n",
    "    \n",
    "    def search_categorical(self):\n",
    "        self.is_categorical = np.zeros(self.X.shape[1]).astype(np.int8)\n",
    "        for j in range(self.X.shape[1]):\n",
    "            uniq = np.unique(self.X[:, j])\n",
    "            if uniq.shape[0] < self.CATEGORICAL_LEN:\n",
    "                self.categorical_vals[j] = uniq\n",
    "                self.is_categorical[j] = 1\n",
    "            \n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # temp sets for comfort\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        # we will store only idxs while training in nodes =>\n",
    "        # make idxs column for comfort:\n",
    "        self.idxs = np.arange(X_train.shape[0])\n",
    "        # for speed search categorical\n",
    "        self.search_categorical()\n",
    "        # build tree \n",
    "        self.build_tree()\n",
    "        # pruning tree\n",
    "        if self.pruning:\n",
    "            self.tree_pruning(self.root)\n",
    "        #  delete temp sets:\n",
    "        #self.free_memory(root)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        del self.idxs\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
    "        # for each elem in X predict result:\n",
    "        for i in np.arange(X_val.shape[0]):\n",
    "            Y_pred[i] = self.predict_one(X_val[i])\n",
    "        return Y_pred\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        node = self.root\n",
    "        while node.is_inner():\n",
    "            if node.predicat(x):\n",
    "                node = node.get_right()\n",
    "            else:\n",
    "                node = node.get_left()\n",
    "        # if in leaf:\n",
    "        return node.get_class()\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Decision Tree\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Tree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод опорных векторов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_SMO:\n",
    "    def __init__(self, X, Y, kernel, C, eps, maxsteps, linear):\n",
    "        self.K = kernel\n",
    "        self.C = C\n",
    "        self.tol = eps\n",
    "        self.maxsteps = maxsteps\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.linear = linear\n",
    "        self.l = np.zeros(Y.shape)\n",
    "        self.e_cache = -Y.astype(np.float64) # zero prediction - y\n",
    "        #self.n = len(X)\n",
    "        self.w0 = 0.0\n",
    "        if linear:\n",
    "            self.w = np.zeros(X.shape[1])\n",
    "\n",
    "\n",
    "    def KKT_violated(self, idx):\n",
    "        r =  self.Y[idx] * self.e_cache[idx]\n",
    "        l = self.l[idx]\n",
    "        # if (M < 1 - tol & l < C) || (M > 1 + tol & l > 0) => KKT violated\n",
    "        # tol - accuracy of computing\n",
    "        return (l < self.C and r < -self.tol) or (l > 0.0 and r > self.tol)\n",
    "\n",
    "    # search by all element ones\n",
    "    def first_heuristic_one(self):\n",
    "        for idx in range(self.l.shape[0]):\n",
    "            if self.KKT_violated(idx):\n",
    "                yield idx\n",
    "\n",
    "    # search by all non-bound elements \n",
    "    def first_heuristic_two(self):\n",
    "        # i think here loop faster then pre-choice by numpy\n",
    "        # because this loop more effective\n",
    "        for idx in range(self.l.shape[0]):\n",
    "            if 0.0 < self.l[idx] < self.C:\n",
    "                if self.KKT_violated(idx):\n",
    "                    yield idx\n",
    "\n",
    "    # search elem that maximaze |E1 - E2| from non-boundary\n",
    "    def second_heuristic_one(self, idx):\n",
    "        # search |E1 - E2| of each elem\n",
    "        dE = np.abs(self.e_cache - self.e_cache[idx])\n",
    "        # all boundary elems not interesting\n",
    "        dE[(self.l >= self.C) | (self.l <= 0.0)] = 0.0\n",
    "        # return i2 = argmax|E1 - E2|\n",
    "        return np.argmax(dE)\n",
    "\n",
    "    def second_heuristic_two(self, idx):\n",
    "        mask = (self.l < self.C) & (self.l > 0.0)\n",
    "        mask[idx] = False\n",
    "        idxes = np.nonzero(mask)[0]\n",
    "        order = np.random.permutation(len(idxes))\n",
    "        return idxes[order]\n",
    "\n",
    "\n",
    "    def second_heuristic_three(self, idx):\n",
    "        #if second heuristic without result, get idxs without elems from second\n",
    "        # it will faster\n",
    "        mask = (self.l >= self.C) | (self.l <= 0.0)\n",
    "        mask[idx] = False\n",
    "        idxes = np.nonzero(mask)[0]\n",
    "        order = np.random.permutation(len(idxes))\n",
    "        return idxes[order]\n",
    "\n",
    "    def get_weights(self):\n",
    "        if self.linear:\n",
    "            return self.w\n",
    "\n",
    "    def get_support(self):\n",
    "        # returns only support vctors with params if you \n",
    "        # dont get them after train\n",
    "        mask = self.l > 0.0\n",
    "        return self.X[mask], self.Y[mask], self.l[mask], self.w0\n",
    "\n",
    "    # return lambdas\n",
    "    def get_coeffs(self):\n",
    "        return self.l\n",
    "\n",
    "    \n",
    "    def optimize_two(self, i1, i2):\n",
    "        # it emulates machine e in computations\n",
    "        eps = 0.00000000000001\n",
    "\n",
    "        y1 = self.Y[i1]\n",
    "        y2 = self.Y[i2]\n",
    "        x1 = self.X[i1]\n",
    "        x2 = self.X[i2]\n",
    "        l1 = self.l[i1]\n",
    "        l2 = self.l[i2]\n",
    "        E1 = self.e_cache[i1]\n",
    "        E2 = self.e_cache[i2]\n",
    "\n",
    "        # compute L H\n",
    "        if y1 == y2:\n",
    "            L = max(0.0, l2 + l1 - self.C)\n",
    "            H = min(self.C, l2 + l1)\n",
    "        else:\n",
    "            L = max(0.0, l2 - l1)\n",
    "            H = min(self.C, self.C + l2 - l1)\n",
    "        if L == H:\n",
    "            return False\n",
    "\n",
    "        # eta\n",
    "        nu = self.K(x1, x1) + self.K(x2, x2) - 2.0*self.K(x1, x2)\n",
    "\n",
    "        # compute l2\n",
    "        if nu > 0.0:\n",
    "            l2 += y2 * (E1 - E2) / nu\n",
    "            if l2 < L:\n",
    "                l2 = L\n",
    "            elif l2 > H:\n",
    "                l2 = H\n",
    "        else:\n",
    "            c1 = nu/2\n",
    "            c2 = y2 * (E1 - E2) + nu * l2\n",
    "            Lobj = c2*L - c1*L*L\n",
    "            Hobj = c2*H - c1*H*H\n",
    "            if Lobj > Hobj + eps:\n",
    "                l2 = L\n",
    "            elif Lobj < Hobj - eps:\n",
    "                l2 = H\n",
    "\n",
    "        if np.abs(l2 - self.l[i2]) < eps*(l2 + self.l[i2] + eps):\n",
    "            return False\n",
    "\n",
    "        # compute l1\n",
    "        l1 -= y1*y2*(l2 - self.l[i2])\n",
    "        \n",
    "        if l1 < 0.0:\n",
    "            l2 += y1*y2*l1\n",
    "            l1 = 0.0\n",
    "        elif l1 > self.C:\n",
    "            l2 += y1*y2*(l1 - self.C)\n",
    "            l1 = self.C\n",
    "\n",
    "        # update w0:\n",
    "        b1 = self.w0 + E1 + y1*(l1 - self.l[i1])*self.K(x1, x1) + y2*(l2 - self.l[i2])*self.K(x1, x2)\n",
    "        b2 = self.w0 + E2 + y1*(l1 - self.l[i1])*self.K(x1, x2) + y2*(l2 - self.l[i2])*self.K(x2, x2)\n",
    "        if l1 > 0.0 and l1 < self.C:\n",
    "            bnew = b1\n",
    "        elif l2 > 0.0 and l2 < self.C:\n",
    "            bnew = b2\n",
    "        else:\n",
    "            bnew = (b1 + b2) / 2.0\n",
    "        \n",
    "        dw0 = bnew - self.w0\n",
    "        self.w0 = bnew\n",
    "\n",
    "        # update E_cache\n",
    "        t1 = y1*(l1 - self.l[i1])\n",
    "        t2 = y2*(l2 - self.l[i2])\n",
    "        \n",
    "        self.e_cache += t1*self.K(self.X, x1) + t2*self.K(self.X, x2) - dw0\n",
    "        \n",
    "        #update w:\n",
    "        if self.linear:\n",
    "            self.w += t1*x1 + t2*x2\n",
    "        # update lambdas:\n",
    "        self.l[i1] = l1\n",
    "        self.l[i2] = l2\n",
    "\n",
    "        return True\n",
    "            \n",
    "            \n",
    "\n",
    "    # search 2 lambdas for optimize and change them\n",
    "    def train(self):\n",
    "        steps = 0\n",
    "        non_bound_loop = True\n",
    "        # main loop\n",
    "        while steps < self.maxsteps:\n",
    "            non_bound_loop ^= True\n",
    "            # outer loops searches i1:\n",
    "            if not non_bound_loop:\n",
    "                # h1-1\n",
    "                changed = False\n",
    "                for idx1 in self.first_heuristic_one():\n",
    "                    # h2-1\n",
    "                    idx2 = self.second_heuristic_one(idx1)\n",
    "                    if self.optimize_two(idx1, idx2):\n",
    "                        changed = True\n",
    "                        continue\n",
    "                    # h2-2,3 together if h2-1 not worked\n",
    "                    # concat idxs of heuristics in sequence\n",
    "                    extra_heuristics = np.concatenate([\n",
    "                        self.second_heuristic_two(idx1),\n",
    "                        self.second_heuristic_three(idx1)\n",
    "                    ])\n",
    "\n",
    "                    for idx2 in extra_heuristics:\n",
    "                        if self.optimize_two(idx1, idx2):\n",
    "                            changed = True\n",
    "                            break\n",
    "                steps += 1\n",
    "                # if nothing changed - work done\n",
    "                if not changed:\n",
    "                    break\n",
    "            else:\n",
    "                # h1-2\n",
    "                # while changing itterate non-bound elements\n",
    "                while changed and steps < self.maxsteps:\n",
    "                    changed = False\n",
    "                    # itterate non-bound elements\n",
    "                    for idx1 in self.first_heuristic_two():\n",
    "                        # h2-1\n",
    "                        idx2 = self.second_heuristic_one(idx1)\n",
    "                        if self.optimize_two(idx1, idx2):\n",
    "                            changed = True\n",
    "                            continue\n",
    "                        # h2-2,3 together if h2-1 not worked\n",
    "                        # concat idxs of heuristics in sequence\n",
    "                        extra_heuristics = np.concatenate([\n",
    "                            self.second_heuristic_two(idx1),\n",
    "                            self.second_heuristic_three(idx1)\n",
    "                        ])\n",
    "                        for idx2 in extra_heuristics:\n",
    "                            if self.optimize_two(idx1, idx2):\n",
    "                                changed = True\n",
    "                                break\n",
    "                    steps += 1\n",
    "        # after train return support vectors with lambdas and Y\n",
    "        mask = self.l > 0.0\n",
    "        return self.X[mask], self.Y[mask], self.l[mask], self.w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return x1.dot(x2.T)\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma):\n",
    "    if len(x1.shape) == 1:\n",
    "        x1r = x1.reshape((1, x1.shape[0]))\n",
    "    else:\n",
    "        x1r = x1\n",
    "    if len(x2.shape) == 1:\n",
    "        x2r = x2.reshape((1, x2.shape[0]))\n",
    "    else:\n",
    "        x2r = x2\n",
    "    ans = np.zeros((x1r.shape[0], x2r.shape[0]))\n",
    "    for i in np.arange(x1r.shape[0]):\n",
    "        # \n",
    "        ans[i] = np.exp(-gamma  * ((x2r - x1r[i])**2).T.sum(axis = 0))\n",
    "    if len(x1.shape) == 1:\n",
    "        ans = ans[0]\n",
    "    if len(x2.shape) == 1:\n",
    "        ans = ans.T[0]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay kernel None for faster linaear version\n",
    "class BinarySVM:\n",
    "    def __init__(self, kernel = None, C=1.0, eps = 0.001, maxsteps=1000):\n",
    "        self.C = C\n",
    "        self.linear = False\n",
    "        if kernel is None:\n",
    "            kernel = lambda x1, x2: x1.dot(x2.T)\n",
    "            self.linear = True\n",
    "        self.K = kernel\n",
    "        self.w = None\n",
    "        self.w0 = None\n",
    "        self.l = None\n",
    "        self.svX = None\n",
    "        self.svY = None\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "    \n",
    "    def weights(self):\n",
    "        if self.linear:\n",
    "            return np.append(self.w0, self.w)\n",
    "        \n",
    "    def predict(self, X_val, border = 0):\n",
    "        # make predict: 0 - negative, 1 - positive\n",
    "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
    "        if self.linear:\n",
    "            x0 = np.ones((X_val.shape[0], 1))\n",
    "            X = np.hstack((x0, X_val))\n",
    "            # <w, x> product for all examples\n",
    "            Xw = X.dot(np.append(-self.w0, self.w))\n",
    "            # a(x) = [<w,x> > t], t - border\n",
    "            Y_pred[Xw >= border] = 1\n",
    "            return Y_pred\n",
    "        else:\n",
    "            yl = (self.svY * self.l).reshape((self.svY.shape[0], 1))\n",
    "            U = self.K(yl * self.svX, X_val).sum(axis = 0) - self.w0\n",
    "            Y_pred[U >= border] = 1\n",
    "            return Y_pred\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        X = X_train\n",
    "        Y = np.array(Y_train)\n",
    "        Y[Y_train == 0] = -1\n",
    "        smo = Binary_SMO(X, Y, self.K, self.C, self.eps, self.maxsteps, self.linear)\n",
    "        self.svX, self.svY, self.l, self.w0 = smo.train()\n",
    "        if self.linear:\n",
    "            self.w = smo.get_weights()\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Support Vector Machine\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SVM\"\n",
    "    \n",
    "    \n",
    "    # returns support vectors with lambdas\n",
    "    def vectors(self):\n",
    "        return self.svX, self.l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод k ближайших соседей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkovski(x1, x2, p = 3):\n",
    "    return (np.abs(x1 - x2) ** p).T.sum(axis = 0)**(1.0 / p)\n",
    "\n",
    "def euclid(x1, x2):\n",
    "    return minkovski(x1, x2, 2)\n",
    "\n",
    "def biquadratical_kernel(t):\n",
    "    return (-t**2 + 1)**2\n",
    "\n",
    "def triquadratical_kernel(t):\n",
    "    return (-t**2 + 1)**3\n",
    "    \n",
    "def triqubical_kernel(t):\n",
    "    return (-t**3 + 1)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay parsen kerel None for simple KNN algo\n",
    "class BinaryKNN:\n",
    "    def __init__(self, k, metric = euclid, parsen_kernel = None):\n",
    "        self.k = k\n",
    "        self.p = metric\n",
    "        # algo shoud remember all data\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        if parsen_kernel is None:\n",
    "            # all elems have equal weight\n",
    "            self.Kp = lambda t: 1.0\n",
    "        else:\n",
    "            # parsen method\n",
    "            self.Kp = parsen_kernel\n",
    "\n",
    "    def stolp_filtration(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        # just remmember data:\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        # check correct of k\n",
    "        self.k = min(self.k, len(self.Y) - 1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        Y_pred = np.zeros(len(X_val)).astype(np.int8)\n",
    "        # for each elem in X predict result:\n",
    "        for i in np.arange(len(X_val)):\n",
    "            Y_pred[i] = self.predict_one(X_val[i])\n",
    "        return Y_pred\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        # compute all distances\n",
    "        r_x = self.p(self.X, x)\n",
    "        # take sorted order of dists in idxs\n",
    "        order = np.argsort(r_x)\n",
    "        # width for parsen is distance to k+1 neiborh:\n",
    "        h = r_x[order[self.k]]\n",
    "        # idxs of first k elems neibrh\n",
    "        order = order[:self.k]\n",
    "        # take first k Y:\n",
    "        Y_k = self.Y[order]\n",
    "        # compute parsen function for all neiborh:\n",
    "        K = self.Kp(r_x[order] / h)\n",
    "        \n",
    "        # compute functional for positives elems\n",
    "        pos_w = (K * Y_k).sum()\n",
    "        # compute functional for negatives elems\n",
    "        neg_w = (K * np.logical_not(Y_k)).sum()\n",
    "        \n",
    "        # class with more functional wins\n",
    "        return int(pos_w > neg_w) # 0 or 1\n",
    "\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"k Nearest Neighbor\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"KNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sklearn реализации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1569,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_cross_validation(model_from_param, param_generator):\n",
    "    max_val = 12\n",
    "    min_val = 10\n",
    "    best_score = 0.0\n",
    "    best_param = None\n",
    "    for p in tqdm(param_generator):\n",
    "        mean_score = 0.0\n",
    "        for alcohol in range(min_val, max_val + 1):\n",
    "            mask_train = alcohol_validate < alcohol\n",
    "            mask_val = alcohol_validate == alcohol\n",
    "            X_t = X_train[mask_train]\n",
    "            Y_t = Y_train[mask_train]\n",
    "            X_v = X_train[mask_val]\n",
    "            Y_v = Y_train[mask_val]\n",
    "            model = model_from_param(p)\n",
    "            model.fit(X_t, Y_t)\n",
    "            mean_score += model.score(X_v, Y_v)\n",
    "        mean_score /= max_val - min_val + 1\n",
    "        if best_score < mean_score:\n",
    "            best_score = mean_score \n",
    "            best_param = p \n",
    "    return best_param, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1658,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter 0.1 \n",
      "Best score 5.974379084967321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.439716312056738"
      ]
     },
     "execution_count": 1658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.01, 0.1)\n",
    "modeling = lambda c: BinaryLogisticRegression(reg_type='l2', C=c, maxsteps=1500, speed=0.02)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best parameter\", best_C, \"\\nBest score\", best_score)\n",
    "\n",
    "my_log_l2_model = BinaryLogisticRegression(reg_type='l2', C=best_C, maxsteps=1500, speed=0.02)\n",
    "my_log_l2_model.fit(X_train, Y_train)\n",
    "my_log_l2_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1659,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter 0.8 \n",
      "Best score 0.6040522875816993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48226950354609927"
      ]
     },
     "execution_count": 1659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.01, 0.1)\n",
    "modeling = lambda c: LogisticRegression(penalty='l2', C=c, max_iter=250)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best parameter\", best_C, \"\\nBest score\", best_score)\n",
    "\n",
    "skl_log_l2_model = LogisticRegression(penalty='l2', C=best_C, max_iter=250)\n",
    "skl_log_l2_model.fit(X_train, Y_train)\n",
    "skl_log_l2_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решающее дерево**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.439716312056738"
      ]
     },
     "execution_count": 1660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_a = 0.00001\n",
    "\n",
    "my_desc_tree_model = BinaryDescisionTree(pruning_cost=best_a)\n",
    "my_desc_tree_model.fit(X_train, Y_train)\n",
    "my_desc_tree_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решающее дерево из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4397163120567376"
      ]
     },
     "execution_count": 1661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_a = 0.000051\n",
    "\n",
    "skl_desc_tree_model = DecisionTreeClassifier(ccp_alpha=best_a)\n",
    "skl_desc_tree_model.fit(X_train, Y_train)\n",
    "skl_desc_tree_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод опорных векторов**\n",
    "\n",
    "Из sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1664,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 27.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter 0.2 \n",
      "Best score 0.43738562091503264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4326241134751773"
      ]
     },
     "execution_count": 1664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.1, 0.1)\n",
    "modeling = lambda c: LinearSVC(C=c)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best parameter\", best_C, \"\\nBest score\", best_score)\n",
    "\n",
    "skl_svm_model = LinearSVC(C=best_C)\n",
    "skl_svm_model.fit(X_train, Y_train)\n",
    "skl_svm_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мой метод опорных векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.439716312056738"
      ]
     },
     "execution_count": 1665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm_model = BinarySVM(kernel=None, C=best_C, maxsteps=1000)\n",
    "my_svm_model.fit(X_train, Y_train)\n",
    "my_svm_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод k ближайших соседей**\n",
    "\n",
    "Из sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 53.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter 16 \n",
      "Best score 0.5194771241830065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46808510638297873"
      ]
     },
     "execution_count": 1662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_k = np.arange(1, 65, 15)\n",
    "modeling = lambda K: KNeighborsClassifier(n_neighbors=K)\n",
    "best_k, best_score = time_cross_validation(modeling, generate_k)\n",
    "print(\"Best parameter\", best_k, \"\\nBest score\", best_score)\n",
    "\n",
    "skl_knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "skl_knn_model.fit(X_train, Y_train)\n",
    "skl_knn_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мой метод k ближайших соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.439716312056738"
      ]
     },
     "execution_count": 1663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_knn_model = BinaryKNN(k=best_k)\n",
    "my_knn_model.fit(X_train, Y_train)\n",
    "my_knn_model.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
